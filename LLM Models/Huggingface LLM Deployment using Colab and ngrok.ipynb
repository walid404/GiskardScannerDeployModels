{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "BhCYv73yOhSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_token =          # Write your ngrok token here\n",
        "!ngrok config add-authtoken {ngrok_token}"
      ],
      "metadata": {
        "id": "2c9c_htQTMXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from fastapi import FastAPI, Request\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = None  # Write your Huggingface token here\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # Write your Huggingface Model Name Here\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    token=hf_token\n",
        ")\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "# Define input schema\n",
        "class Prompt(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "async def generate_text(prompt: Prompt):\n",
        "    input_ids = tokenizer(prompt.text, return_tensors=\"pt\").input_ids\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_new_tokens=100)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return {\"response\": response}\n"
      ],
      "metadata": {
        "id": "ovsd42mHTMXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "# Run FastAPI app in a separate thread\n",
        "def run_app():\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n",
        "\n",
        "# Start ngrok tunnel\n",
        "time.sleep(180) # adjust the time until the model loaded successfully before running the next cell"
      ],
      "metadata": {
        "id": "KVBNoM74TMXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "id": "BN2m_TxCTMXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}