{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIMXnK2cJI6i"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAYpBx7BKTNa"
      },
      "outputs": [],
      "source": [
        "ngrok_token =           # Write your ngrok token here\n",
        "!ngrok config add-authtoken {ngrok_token}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5NxrYp0tZ2a"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwuZpGqGLwvX",
        "outputId": "bd417c4b-1205-430a-e5cb-3485b0e1f6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# === FastAPI app === #\n",
        "app = FastAPI()\n",
        "\n",
        "# ========================= You Can Update From Here ========================= #\n",
        "\n",
        "# === Load trained model and training features === #\n",
        "with open(\"classifier.pkl\", \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "\n",
        "# === Constants for binning === #\n",
        "AGE_BINS = [0, 19, 29, 39, 49, 59, 69, 80]\n",
        "AGE_CATEGORIES = [\"<20s\", \"20s\", \"30s\", \"40s\", \"50s\", \"60s\", \">60s\"]\n",
        "\n",
        "NA_TO_K_BINS = [0, 9, 19, 29, 50]\n",
        "NA_TO_K_CATEGORIES = [\"<10\", \"10-20\", \"20-30\", \">30\"]\n",
        "\n",
        "features_model = [\"Age\", \"Sex\", \"BP\", \"Cholesterol\", \"Na_to_K\"] # Ensure the order matches the model training process\n",
        "\n",
        "# === Input schema === #\n",
        "class InputData(BaseModel):\n",
        "    Age: float\n",
        "    Sex: str\n",
        "    BP: str\n",
        "    Cholesterol: str\n",
        "    Na_to_K: float\n",
        "\n",
        "# === Preprocessing functions === #\n",
        "def preprocessing(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Perform binning for Age and Na_to_K.\"\"\"\n",
        "    # Ensure that all NaN values are properly handled prior to sending the CSV\n",
        "    # to the scanner â€” this hasnâ€™t been done here.\n",
        "\n",
        "    df = df.copy()\n",
        "    df['Age'] = pd.cut(df['Age'], bins=AGE_BINS, labels=AGE_CATEGORIES, right=False, include_lowest=True)\n",
        "    df['Na_to_K'] = pd.cut(df['Na_to_K'], bins=NA_TO_K_BINS, labels=NA_TO_K_CATEGORIES, right=False, include_lowest=True)\n",
        "    return df\n",
        "\n",
        "# ================================ Until Here ================================ #\n",
        "\n",
        "\n",
        "# === Prediction endpoint === #\n",
        "@app.post(\"/predict_point\")\n",
        "async def predict(data: InputData):\n",
        "    try:\n",
        "        # Step 1: Create DataFrame\n",
        "        df = pd.DataFrame([data.dict()])\n",
        "        df = df[features_model]\n",
        "\n",
        "        # Step 2: preprocess\n",
        "        df = preprocessing(df)\n",
        "\n",
        "        # Step 3: Predict\n",
        "        prediction = model.predict_proba(df) # Must return the probabilities\n",
        "\n",
        "        return JSONResponse(content={\"predictions\": prediction[0].tolist()}, status_code=200) # Don't Change the predictions names in JSONResponse\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "# === Batch Prediction endpoint === #\n",
        "@app.post(\"/predict_batch\")\n",
        "async def predict(data: List[InputData]):\n",
        "    try:\n",
        "        # Convert list of InputData into DataFrame\n",
        "        df = pd.DataFrame([item.dict() for item in data])\n",
        "\n",
        "        # Preprocess\n",
        "        df = df[features_model]\n",
        "        df = preprocessing(df)\n",
        "\n",
        "\n",
        "        # Predict\n",
        "        predictions = model.predict_proba(df) # Must return the probabilities\n",
        "\n",
        "        # Convert to list of lists\n",
        "        return JSONResponse(content={\"predictions\": predictions.tolist()}, status_code=200) # Don't Change the predictions names in JSONResponse\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "# Start Ngrok and server\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"ðŸš€ Public URL: {public_url}\")\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UurXao5svuc8"
      },
      "outputs": [],
      "source": [
        "!python main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfdxCj1v5dLQ"
      },
      "source": [
        "### Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay9rW-v2WzRc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Union\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# === FastAPI app === #\n",
        "app = FastAPI()\n",
        "\n",
        "# ========================= You Can Update From Here ========================= #\n",
        "\n",
        "\n",
        "# === Load Model === #\n",
        "model = joblib.load(\"regressor.pkl\")\n",
        "\n",
        "# === Features used in model === #\n",
        "features_model = [\n",
        "    'battery_capacity_kWh', 'efficiency_wh_per_km', 'drivetrain',\n",
        "    'fast_charging_power_kw_dc', 'car_body_type', 'torque_nm',\n",
        "    'acceleration_0_100_s', 'towing_capacity_kg', 'seats',\n",
        "    'length_mm', 'width_mm', 'height_mm'\n",
        "] # Ensure the order matches the model training process\n",
        "\n",
        "\n",
        "# === Input Schema === #\n",
        "class CarFeatures(BaseModel):\n",
        "    battery_capacity_kWh: float\n",
        "    efficiency_wh_per_km: float\n",
        "    drivetrain: str\n",
        "    fast_charging_power_kw_dc: float\n",
        "    car_body_type: str\n",
        "    torque_nm: float\n",
        "    acceleration_0_100_s: float\n",
        "    towing_capacity_kg: float\n",
        "    seats: float\n",
        "    length_mm: float\n",
        "    width_mm: float\n",
        "    height_mm: float\n",
        "\n",
        "# === Preprocessing === #\n",
        "def preprocess(df):\n",
        "    # Ensure that all NaN values are properly handled prior to sending the CSV\n",
        "    # to the scanner â€” this hasnâ€™t been done here.\n",
        "\n",
        "    label_cols = ['drivetrain', 'car_body_type']\n",
        "    for col in label_cols:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "    return df\n",
        "\n",
        "# ================================ Until Here ================================ #\n",
        "\n",
        "\n",
        "# === Endpoint: Predict Single === #\n",
        "@app.post(\"/predict_point\")\n",
        "async def predict_one(data: CarFeatures):\n",
        "    try:\n",
        "        df = pd.DataFrame([data.dict()])\n",
        "        df = df[features_model]\n",
        "        df = preprocess(df)\n",
        "        prediction = model.predict(df)\n",
        "        return JSONResponse(content={\"predictions\": prediction[0]}, status_code=200) # Don't Change the predictions names in JSONResponse\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "# === Endpoint: Predict Batch === #\n",
        "@app.post(\"/predict_batch\")\n",
        "async def predict_batch(data: List[CarFeatures]):\n",
        "    try:\n",
        "        df = pd.DataFrame([d.dict() for d in data])\n",
        "        df = df[features_model]\n",
        "        df = preprocess(df)\n",
        "        predictions = model.predict(df)\n",
        "        return JSONResponse(content={\"predictions\": predictions.tolist()}, status_code=200) # Don't Change the predictions names in JSONResponse\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "# === Run with ngrok === #\n",
        "if __name__ == \"__main__\":\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"ðŸš€ Public URL: {public_url}\")\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh98CvAf4Qnl"
      },
      "outputs": [],
      "source": [
        "!python main.py"
      ]
    }
  ]
}